# Azure Banking ETL Pipeline ğŸš€

This project simulates a real-world banking data pipeline using Azure cloud services and open-source tools. It demonstrates an end-to-end ETL (Extract, Transform, Load) process for ingesting, transforming, and analyzing banking datasets using:

- **Azure Data Factory** for orchestrating data pipelines
- **Azure Data Lake Storage** for raw/staged/processed layers
- **Azure Databricks / PySpark** for transformations
- **Azure Synapse SQL Serverless** for querying data lake with SQL
- **Delta Lake format** for scalable and ACID-compliant storage

---

## ğŸ“ Folder Structure

```
azure-banking-etl-pipeline/
â”œâ”€â”€ adf-pipeline/            # ADF JSON definition
â”œâ”€â”€ architecture/            # Diagrams and design artifacts
â”œâ”€â”€ datasets/                # Sample input CSVs
â”œâ”€â”€ notebooks/               # PySpark scripts (Databricks-ready)
â”œâ”€â”€ synapse-sql/             # SQL views for Synapse Serverless
â”œâ”€â”€ docs/                    # Walkthroughs and use cases
â””â”€â”€ README.md                # Youâ€™re here!
```

---

## ğŸ¦ Use Case

A fictional bank needs to modernize its legacy data pipeline. This solution:
- Migrates customer and transaction data to Azure
- Cleans & enriches records using PySpark
- Stores output in Delta format
- Enables analytics using Synapse SQL views

---

## ğŸ› ï¸ Tools Used

- Azure Data Factory
- Azure Data Lake Gen2
- Azure Synapse Analytics (SQL Serverless)
- Azure Databricks
- PySpark
- Delta Lake

---

## ğŸ“Š Sample Outputs

- Identify high-value customers
- Aggregate spend by region
- Join transactions with customer metadata

---

## ğŸ§  Learning Outcomes

- Hands-on orchestration with ADF
- Modular PySpark scripts for batch ETL
- SQL-based reporting on Delta tables
